{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sequential 모델을 넘어서: 케라스 함수형 API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 지금까지 이 책에서 소개한 모든 신경망은 Sequential 모델을 사용하여 만들어졌습니다. Sequential 모델은 네트워크 입력과 출력이 하나라고 가정합니다. 이 모델은 층을 차례대로 쌓아 구성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하지만 일부 네트워크는 개별 입력이 여러 개 필요하거나 출력이 여러 개 필요합니다. 층을 차례대로 쌓지 않고 층 사이를 연결하여 그래프처럼 만드는 네트워크도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여러 경우에 다중 입력 모델, 다중 출력 모델, 그래프 구조를 띤 모델이 필요하지만 케라스의 Sequential 클래스를 사용해서는 만들지 못합니다. 케라스에는 훨씬 더 일반적이고 유연한 다른 방법인 함수형 API가 있다. 이 절에서 함수형 API가 무엇인지 소개하고 함수형 API를 사용하는 방법과 이를 할 수 있는 것을 자세히 설명하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 함수형 API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 함수형 API에서는 직접 텐서들의 입출력을 다룹니다. 함수처럼 층을 사용하여 텐서를 입력받고 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Input, layers\n",
    "from keras.datasets import mnist\n",
    "# 함수형 API는 직접 텐서들의 입출력을 다룹니다. 함수처럼 층을 사용하여 텐서를 입력받고 출력합니다.\n",
    "\n",
    "input_tensor = Input(shape=(32,))\n",
    "dense = layers.Dense(32, activation='relu')\n",
    "output_tensor = dense(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 3,466\n",
      "Trainable params: 3,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "\n",
    "seq_model = Sequential()\n",
    "seq_model.add(layers.Dense(32, activation='relu', input_shape=(64,)))\n",
    "seq_model.add(layers.Dense(32, activation='relu'))\n",
    "seq_model.add(layers.Dense(10, activation='softmax'))\n",
    "seq_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 3,466\n",
      "Trainable params: 3,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# My code\n",
    "input_tensor = Input(shape=(64,))\n",
    "dense1 = layers.Dense(32, activation='relu')\n",
    "dense2 = layers.Dense(32, activation='relu')\n",
    "output = layers.Dense(10, activation='softmax')\n",
    "\n",
    "fc1 = dense1(input_tensor)\n",
    "fc2 = dense2(fc1)\n",
    "output_tensor = output(fc2)\n",
    "\n",
    "model = Model(input_tensor, output_tensor)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(64,))\n",
    "x = layers.Dense(32, activation='relu')(input_tensor)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 3,466\n",
      "Trainable params: 3,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(input_tensor, output_tensor)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = np.random.random((1000, 64))\n",
    "y_train = np.random.random((1000, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 303us/step - loss: 11.5110\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 11.5021\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 11.4996\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 11.4971\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 11.4946\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 11.4920\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 11.4900\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 11.4884\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 11.4867\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 11.4849\n",
      "1000/1000 [==============================] - 0s 61us/step\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=128)\n",
    "score = model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.4808760223\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 다중 입력 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다중 API는 다중 입력 모델을 만드는 데 사용할 수 있다. 일반적으로 이런 모델은 다른 입력 가지를 합치기 위해 여러 텐서를 연결할 수 있는 층을 사용합니다. 텐서를 더하거나 이어 붙입니다. 이와 관련된 케라스 함수는 keras.layers.add, keras.layers.concatenate입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 질문 응답 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전형적인 질문 응답 모델은 2개의 입력을 가진다. 하나는 자연어 질문이고, 또 하나는 답변에 필요한 정보가 담겨 있는 텍스트입니다. 그러면 모델은 답을 출력해야 합니다. 가장 간단한 구조는 미리 정의한 어휘 사전에서 소프트맥스 함수를 통해 한 단어로 된 답을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "\n",
    "text_vocabulary_size = 10000\n",
    "question_vocabulary_size = 10000\n",
    "answer_vocabulary_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력의 크기가 64인 벡터 시퀀스로 임베딩한다.\n",
    "text_input = Input(shape=(None,), dtype='int32', name='text')\n",
    "embedded_text = layers.Embedding(text_vocabulary_size, 64)(text_input)\n",
    "encoded_text = layers.LSTM(32)(embedded_text)\n",
    "\n",
    "# 입력의 크기가 64인 백터 시퀀스로 임베딩한다.\n",
    "question_input = Input(shape=(None,), dtype='int32', name='question')\n",
    "embedded_question = layers.Embedding(question_vocabulary_size, 64)(question_input)\n",
    "encoded_question = layers.LSTM(16)(embedded_question)\n",
    "\n",
    "# concanenate\n",
    "concatenated = layers.concatenate([encoded_text, encoded_question], axis=-1)\n",
    "\n",
    "# answer\n",
    "answer = layers.Dense(answer_vocabulary_size, activation='softmax')(concatenated)\n",
    "\n",
    "# model concatenation\n",
    "model = Model([text_input, question_input], answer)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text (InputLayer)               (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "question (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, None, 64)     640000      text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, None, 64)     640000      question[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 32)           12416       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 16)           5184        embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 48)           0           lstm_5[0][0]                     \n",
      "                                                                 lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 500)          24500       concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,322,100\n",
      "Trainable params: 1,322,100\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from keras.utils import to_categorical\n",
    "\n",
    "num_samples = 1000\n",
    "max_length = 100\n",
    "\n",
    "text = np.random.randint(1, text_vocabulary_size, size=(num_samples, max_length))\n",
    "question = np.random.randint(1, question_vocabulary_size, size=(num_samples, max_length))\n",
    "answers = np.random.randint(0, answer_vocabulary_size, size=num_samples)\n",
    "answer = to_categorical(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 100), (1000, 100), (1000,), (1000, 500))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.shape, question.shape, answers.shape, answer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 6.2145 - acc: 1.0000e-03\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 6.1962 - acc: 0.0410\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 6.1444 - acc: 0.0110\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 6.0479 - acc: 0.0100\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 5.9588 - acc: 0.0070\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 5.8404 - acc: 0.0090\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 5.7599 - acc: 0.0200\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 5.6828 - acc: 0.0320\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 5.6090 - acc: 0.0360\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 5.5391 - acc: 0.0530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x6dbff2c390>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([text, question], answer, epochs=10, batch_size=128)\n",
    "\n",
    "# dictionary input\n",
    "# model.fit({'text': text, 'question': question}, answers, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 다중 출력 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 소셜 미디어에서 익명 사용자의 포스트를 입력으로 받아 그 사람의 나이, 성별, 소득 수준을 예측으로 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "\n",
    "vocabulary_size = 50000\n",
    "num_income_groups = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "posts (InputLayer)              (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 500, 256)     12800000    posts[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 496, 128)     163968      embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 99, 128)      0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 95, 256)      164096      max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 91, 256)      327936      conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 18, 256)      0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 14, 256)      327936      max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 10, 256)      327936      conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 256)          0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 128)          32896       global_max_pooling1d_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "age (Dense)                     (None, 1)            129         dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "income (Dense)                  (None, 10)           1290        dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "gender (Dense)                  (None, 1)            129         dense_19[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 14,146,316\n",
      "Trainable params: 14,146,316\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "posts_input = Input(shape=(None,), dtype='int32', name='posts')\n",
    "\n",
    "# input_length를 안 정하면 conv1d가 무엇을 해야 할 지 모른다.\n",
    "embedded_posts = layers.Embedding(vocabulary_size, 256, input_length=max_length)(posts_input)\n",
    "\n",
    "# parameter 사이즈: 256 x 128 x 5 + 128\n",
    "x = layers.Conv1D(128, 5, activation='relu')(embedded_posts)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "age_prediction = layers.Dense(1, name='age')(x) # 나이\n",
    "income_prediction = layers.Dense(num_income_groups, activation='softmax', name='income')(x)\n",
    "gender_prediction = layers.Dense(1, activation='sigmoid', name='gender')(x)\n",
    "\n",
    "# model\n",
    "model = Model(posts_input, [age_prediction, income_prediction, gender_prediction])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각각의 층마다 손실 함수가 다르다.\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss=['mse', 'categorical_crossentropy', 'binary_crossentropy'])\n",
    "\n",
    "# model.compile(optimizer='rmsprop',\n",
    "#              loss={'age': 'mse',\n",
    "#                    'income': 'categorical_crossentropy',\n",
    "#                    'gender': 'binary_crossentropy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 경사 하강법은 하나의 스칼라 값을 최소화 하기 때문에 모델을 훈련하려면 이 손실들을 하나의 값으로 합쳐야 합니다. 손실 값을 합치는 가장 간단한 방법은 모두 더하는 것입니다. 케라스에서는 compile 메서드에 리스트나 딕셔너리를 사용하여 출력마다 다른 손실을 지정할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 손실 값이 많이 불균형하면 모델이 개별 손실이 갖아 큰 작업에 치우쳐 표현을 최적화할 것입니다. 그 결과 다른 작업은 손해를 봅니다. 이를 해결하기 위해 손실 값이 최종 손실에 기여하는 수준을 지정할 수 있습니다. 특히 손실 값의 스케일이 다를 때 유용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 나이 회귀 작업에 사용되는 평균 제곱 오차 손실은 일반적으로 3 ~ 5 사이 값을 가집니다. 반면에 성별 분류 작업에 사용되는 크로스엔트로피 손실은 0.1 정도로 낮습니다. 이런 환경에서 손실에 균형을 맞추려면 크로스엔트로피 손실에 가중치 10을 주고 mse 손실에 가중치 0.25를 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "max_length = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = np.random.randint(1, vocabulary_size, size=(num_samples, max_length))\n",
    "ages = np.random.randint(1, 100, size=num_samples)\n",
    "income = np.random.randint(1, 10, size=num_samples)\n",
    "gender = np.random.randint(0, 1, size=num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_targets = ages\n",
    "income_targets = to_categorical(income)\n",
    "gender_targets = to_categorical(gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 500), (10000,), (10000, 10), (10000, 1))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.shape, age_targets.shape, income_targets.shape, gender_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 6s 558us/step - loss: 291.3245 - age_loss: 1155.2206 - income_loss: 2.3870 - gender_loss: 0.0132\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 3s 342us/step - loss: 208.4401 - age_loss: 824.6087 - income_loss: 2.2878 - gender_loss: 1.8840e-05\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 3s 340us/step - loss: 111.3566 - age_loss: 436.2864 - income_loss: 2.2828 - gender_loss: 2.2562e-04\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 4s 363us/step - loss: 71.3523 - age_loss: 276.3160 - income_loss: 2.2725 - gender_loss: 7.3663e-05\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 3s 338us/step - loss: 60.6190 - age_loss: 233.3628 - income_loss: 2.2782 - gender_loss: 1.2472e-05\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 3s 340us/step - loss: 50.5735 - age_loss: 193.2026 - income_loss: 2.2728 - gender_loss: 1.6418e-06\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 3s 338us/step - loss: 42.5765 - age_loss: 161.2091 - income_loss: 2.2742 - gender_loss: 1.1603e-06\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 3s 345us/step - loss: 38.7857 - age_loss: 146.0272 - income_loss: 2.2789 - gender_loss: 6.0538e-07 1s - loss: 39.1781 - age_loss: 147.5874 - income_loss: 2.2813 - gender_loss: 8.5352 - ETA: 1s - loss: 38.4663 - age_loss: 144.7432 - income_loss: 2.2805 - gender_loss: 7.7966e- - ETA: 0s - loss: 38.5899 - age_loss: 145.2376 - income_loss: 2.2805 - gender_lo\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 3s 340us/step - loss: 34.1576 - age_loss: 127.5501 - income_loss: 2.2701 - gender_loss: 2.3291e-07\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 3s 339us/step - loss: 30.8985 - age_loss: 114.4798 - income_loss: 2.2786 - gender_loss: 4.8456e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xff0351ba90>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss=['mse', 'categorical_crossentropy', 'binary_crossentropy'],\n",
    "              loss_weights=[0.25, 1., 10.])\n",
    "model.fit(posts, [age_targets, income_targets, gender_targets], epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 층으로 구성된 비순환 유향 그래프"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 함수형 API를 사용하면 다중 입력이나 다중 출력뿐만 아니라 내부 토폴로지가 복잡한 네트워크도 만들 수 있다. 케라스의 신경망은 층으로 구성된 어떤 비순환 유향 그래프도 만들 수 있습니다. 비순환이라는 것이 중요합니다. 다시 말해 이 그래프는 원형을 띨 수 없습니다. 텐서 x가 자기 자신을 출력하는 층의 입력이 될 수 없습니다. 만들 수 있는 루프는 순환 층의 내부에 있는 것뿐입니다.\n",
    "\n",
    "- 그래프로 구현된 몇 개 신경망 컴포넌트가 널리 사용됩니다. 가장 유명한 2개는 인셉션 모듈과 잔차 연결입니다. 케라스에서 이 2개의 컴포넌트를 어떻게 구현하는지 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 인셉션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(256, 256, 3), dtype='float32')\n",
    "\n",
    "# first route\n",
    "r1 = layers.Conv2D(64, (1, 1), strides=2, activation='relu')(input_tensor)\n",
    "\n",
    "# second route\n",
    "r2 = layers.Conv2D(32, (1, 1), strides=1, activation='relu', padding='same')(input_tensor)\n",
    "r2 = layers.Conv2D(64, (3, 3), strides=2, activation='relu', padding='same')(r2)\n",
    "\n",
    "# third route\n",
    "r3 = layers.AveragePooling2D(pool_size=(3, 3), strides=2, padding='same')(input_tensor)\n",
    "r3 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(r3)\n",
    "\n",
    "r4 = layers.Conv2D(32, (1, 1), activation='relu')(input_tensor)\n",
    "r4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(r4)\n",
    "r4 = layers.Conv2D(64, (3, 3), activation='relu', strides=2, padding='same')(r4)\n",
    "\n",
    "output_tensor = layers.concatenate([r1, r2, r3, r4], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1x1 convolution: 채널 공간 방향으로 상관관계가 크고 채널 간에는 독립적이라고 가정하면 납득할만한 전략이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 잔차 연결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대규모 딥러닝 모델에서 흔히 나타나는 2가지 문제인 그래디언트 소실과 표현 병목을 해결했습니다.\n",
    "\n",
    "- 하위 층의 출력이상위 층의 활성화 출력에 연결되는 것이 아니고 더해집니다. 따라서 두 출력의 크기가 동일해야 합니다.\n",
    "\n",
    "- 잔차 연결은 하위 층의 출력을 상위 층의 입력으로 사용합니다. 순서대로 놓인 네트워크를 질러가는 연결이 만들어진다. 하위 층의 출력이 상위 층의 활성화 출력에 연결되는 것이 아니고 더해집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "x = Input(shape=(224, 224, 64))\n",
    "x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "\n",
    "# concatenate이 아니라 더해진다.\n",
    "y = layers.add([y, x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_4/add:0' shape=(?, 224, 224, 128) dtype=float32>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "y = layers.MaxPooling2D(2, strides=2)(y)\n",
    "\n",
    "residual = layers.Conv2D(128, 1, strides=2, padding='same')(x)\n",
    "y = layers.add([y, residual])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_5/add:0' shape=(?, 112, 112, 128) dtype=float32>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 표현 병목\n",
    "\n",
    "- 딥러닝 층은 이전 층의 활성화 정보만 사용합니다. 어떤 층이 너무 작으면 이 활성화 출력에 얼마나 많은 정보를 채울 수 있느냐에 모델 성능이 좌우된다. 이 개념을 신호 처리에 비유할 수 있습니다. 일련의 연산으로 구성된 오디오 처리 파이프라인이 있다고 해보자. 각 단계는 이전 연산의 출력을 입력으로 사용합니다. 한 연산이 신호를 저주파 영역으로 잘라냈다면 이후 연산이 줄어든 주파수를 복원할 수 없을 것이다. 손실된 정보는 영구 불변이다. 하위 층의 정보를 다시 주입하는 잔차 연결은 딥러닝 모델에서 이 이슈를 어느 정도 해결합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝의 그래디언트 소실 문제\n",
    "\n",
    "- 피드백 신호가 깊이 쌓인 층을 통과하여 전파되면 신호가 아주 작아지거나 완전히 사라질 수 있다. 이렇게 되면 네트워크가 훈련되지 않는다. 이를 그래디 언트 소실이라 한다. 이 문제는 심층 신경망과 긴 시퀀스를 처리하는 순환 신경망에서 모두 나타납니다. 양쪽 모두 피드백 신호가 일련의 긴 연산을 통과하여 전파되기 때문입니다.\n",
    "\n",
    "- 순환 신경망에서 LSTM 층이 이 문제를 해결하기 위해 사용하는 방식을 보았습니다. 이동 트랙이 주요 처리 트랙에 나란한 정보를 실어 날랐습니다. 잔차 연결은 피드포워드 신경망에서 비슷한 역할을 합니다. 하지만 좀 더 단순합니다. 주 네트워크 층에 나란히 단순한 선형 정보를 실어 나릅니다. 이는 그래디언트가 깊게 쌓인 층을 통과하여 전파하도록 도와줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 층 가중치 공유"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 함수형 API의 중요한 또 하나의 기능은 층 객체를 여러 번 재사용할 수 있다는 것입니다. 층 객체를 두 번 호출하면 새로운 층 객체를 만들지 않고 각 호출에 동일한 가중치를 재사용합니다. 이런 기능 때문에 공유 가지를 가진 모델을 만들 수 있습니다. 이런 가지는 같은 가중치를 공유하고 같은 연산을 수행합니다. 다시 말해 같은 표현을 공유하고 이런 표현을 다른 입력에서 함께 학습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예를 들어 두 문장 사이 의미가 비슷한지 측정하는 모델을 가정한다. 이 모델은 2개의 입력을 받고 0과 1 사이의 점수를 출력합니다. 0은 관련 없는 문장을 의미하고 1은 두 문장이 동일하거나 재구성되었다는 것을 의미한다. 이런 모델은 대화 시스템에서 자연어 질의에 대한 중복 제거를 포함하여 많은 어플리케이션에서 유용하게 사용될 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이런 문제에서는 두 입력 시퀀스가 바뀔 수 있습니다. 의미가 비슷하다는 것은 대칭적인 관계이기 때문입니다. A에서 B에 대한 유사도는 B에서 A에 대한 유사도와 같습니다. 이런 이유 때문에 각 입력 문장을 처리하는 2개의 독립된 모델을 학습하는 것은 이치에 맞지 않습니다. 그 대신 LSTM 층으로 양쪽을 모두 처리하는 것이 좋습니다. 이 LSTM 층의 표현은 두 입력에 대해 학습됩니다. 이를 샴 LSTM이 합니다. 또는 공유 LSTM이라 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           (None, None, 128)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           (None, None, 128)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 32)           20608       input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 64)           0           lstm_7[0][0]                     \n",
      "                                                                 lstm_7[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 1)            65          concatenate_13[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 20,673\n",
      "Trainable params: 20,673\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "\n",
    "# 새로운 층 객체를 만드는 것이 아니라 각 호출에 동일한 가중치를 재사용한다.\n",
    "lstm = layers.LSTM(32)\n",
    "\n",
    "# 모델의 왼쪽 가지를 구성합니다. 입력은 크기가 128인 벡터의 가변 길이 시퀀스입니다.\n",
    "left_input = Input(shape=(None, 128))\n",
    "left_output = lstm(left_input)\n",
    "\n",
    "# 모델의 오른쪽 가지를 구성합니다. 기존 층 객체를 호출하면 가중치가 재샤용됩니다.\n",
    "right_input = Input(shape=(None, 128))\n",
    "right_output = lstm(right_input)\n",
    "\n",
    "merged = layers.concatenate([left_output, right_output], axis=-1)\n",
    "\n",
    "# 맨 위에 분류기를 놓습니다.\n",
    "predictions = layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "# 모델 객체를 만들고 훈련합니다. 이런 모델을 훈련하면 LSTM 가중치는 양쪽 입력을 바탕으로 업데이트됩니다.\n",
    "model = Model([left_input, right_input], predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 층과 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 함수형 API에서는 모델을 층처럼 사용할 수 있습니다. 모델을 '커다란 층'으로 생각해도 됩니다. Sequential 클래스와 Model 클래스에서 모두 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = modle(x)\n",
    "# y1, y2 = model([x1, x2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 객체를 호출할 때 모델 가중치가 재사용됩니다. 층 객체를 호출할 때와 정확히 같습니다. 층 객체나 모델 객체나 객체를 호출하는 것은 항상 그 객체가 가진 학습된 표현을 재사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 듀얼 카메라에서 입력을 받는 비전 모델의 경우에 왼쪽 카메라와 오른쪽 카메라에서 시각적 특징을 추출하여 합칠 떄 2개의 독립된 모델을 사용할 필요가 없습니다. 두 입력에 저수준 처리 과정이 공유될 수 있다. 다시 말해 가중치가 같고 동일한 표현을 공유하는 층을 사용합니다. 다음은 케라스에서 샴 비전 모델을 구현하는 예입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import applications\n",
    "from keras import Input\n",
    "\n",
    "xception_base = applications.Xception(weights=None, include_top=False)\n",
    "\n",
    "left_input = Input(shape=(250, 250, 3))\n",
    "right_input = Input(shape=(250, 250, 3))\n",
    "\n",
    "left_features = xception_base(left_input)\n",
    "right_features = xception_base(right_input)\n",
    "\n",
    "merged_features = layers.concatenate([left_features, right_features], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 차례대로 층을 쌓는 것 이상이 필요할 때는 Sequential API를 사용하지 않는다.\n",
    "- 함수형 API를 사용하여 다중 입력, 다중 출력, 봊갑한 네트워크 토폴로지를 갖는 케라스 모델을 만드는 방법\n",
    "- 다른 네트워크 가지에서 같은 층이나 모델 객체를 여러 번 호출하여 가중치를 재사용하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
